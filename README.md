# Representation

  Time: 19:00 pm, Tuesday

  Venue: JS104

  Welcome to AntNLP Seminar 2020 Spring. :)

# On Papers
 * Please choose recent papers (2020, 2019, 2018) from top NLP/AI venues. A (incomplete) list is

    * NLP: ACL, TACL, EMNLP, NAACL, EACL, COLING
    * ML: ICML, NeurIPS, AISTATS, JMLR, ICLR, ICDE
    * AI: AAAI, IJCAI, CVPR
    * IR/DM: SIGIR, CIKM, WSDM, KDD, WWW, ICDM
 
* While we are interested in a broad range of NLP/AI topics, the followings (and a list here) are of great importance

    * syntactic/semantic parsing
    * entity/relation/event extraction
    * distributed/distributional/compositional semantics
    * MT/MRC/QA/Dialog
    * knowledge analysis and processing
    * (deep) learning algorithms

* Materials with broad interests are welcome (e.g., tutorials form top conferences, high-quality surveys).

# For Presenters

 * Please fill your slots in the Agenda at least one week before your presentation.
    * Please format Paper fields with [venue+year]title (e.g. [ACL19]A Good Paper).
    * Please upload your slides, and add links to them in Slides fields.
 * Besides technical novelties, please give enough background knowledge in case people are unfamiliar with your topic.
 * It would be great to keep your presentation within 50 min.

# For Audiences
 * Please read abstract/introduction sections before the seminar.
 
# Agenda

| Week | Date | Name | Paper | Materials |
|------|------|------|-------|-----------|
|7|2020-11-3|宋扬|[AAAI16]Text Matching as Image Recognition|[slides](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Text%20Matching%20as%20Image%20Recognition.pdf)|



# Alternative Reading List

If you have no idea on paper selection, you can alternatively choose a paper from the following list for representation.

| Paper | Materials |
|-|-|
| Neural Machine Reading Comprehension: Methods and Trends | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Neural%20Machine%20Reading%20Comprehension%20Methods%20and%20Trends.pdf) |
| Neural Reading Comprehension and Beyond |  [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Neural%20Reading%20Comprehension%20and%20Beyond.pdf) |
| Attention-over-Attention Neural Networks for Reading Comprehension | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Attention-over-Attention%20Neural%20Networks%20for%20Reading%20Comprehension.pdf) |
| FANG: Leveraging Social Context for Fake News Detection Using Graph Representation | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Leveraging%20Social%20Context%20for%20Fake%20News%20Detection.pdf)|
| Sequence to Sequence Learning with Neural Networks | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.pdf) |
| Generative Adversarial Nets | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Generative%20Adversarial%20Nets.pdf) |
| Attention is All You Need | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/attention%20is%20all%20you%20need.pdf) |
| Language Models are Unsupervised Multitask Learners  | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/language%20models%20are%20unsupervised%20multitask%20learners.pdf) |
| A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/A%20Minimax%20Game%20for%20Unifying%20Generative%20and%20Discriminative%20Information%20Retrieval%20Models.pdf) |
| Pre-training of Deep Bidirectional Transformers for Language Understanding | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.pdf) |
| Multi-task Deep Neural Networks for Natural Language Undestandering | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/multi-task%20deep%20neural%20networks%20for%20natural%20language%20undestandering.pdf) |
| Distributed Representations of Words and Phrases and Their Compositionality | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Distributed%20representations%20of%20words%20and%20phrases%20and%20their%20compositionality.pdf) |
| Efficient estimation of word representations in vector space | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Efficient%20estimation%20of%20word%20representations%20in%20vector%20space.pdf) |
| Playing Atari with Deep Reinforcement Learning  | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Playing%20Atari%20with%20Deep%20Reinforcement%20Learning.pdf) |
| Human Level Control through Deep Reinforcement Learning  | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Human%20Level%20Control%20through%20Deep%20Reinforcement%20Learning.pdf) |


# F.A.Q.

* How to fill the slots and upload your slides?
   * creating-a-pull-request-from-a-fork/
   * or you can contact: Longfei Zhang
* any quesitons, please feel free to contact us.

