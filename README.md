# Representation

  Time: 17:00 pm, Tuesday

  Venue: JS104

  Welcome to AntNLP Seminar 2020 Spring. :)

# On Papers
 * Please choose recent papers (2020, 2019, 2018) from top NLP/AI venues. A (incomplete) list is

    * NLP: ACL, TACL, EMNLP, NAACL, EACL, COLING
    * ML: ICML, NeurIPS, AISTATS, JMLR, ICLR, ICDE
    * AI: AAAI, IJCAI, CVPR
    * IR/DM: SIGIR, CIKM, WSDM, KDD, WWW, ICDM
 
* While we are interested in a broad range of NLP/AI topics, the followings (and a list here) are of great importance

    * syntactic/semantic parsing
    * entity/relation/event extraction
    * distributed/distributional/compositional semantics
    * MT/MRC/QA/Dialog
    * knowledge analysis and processing
    * (deep) learning algorithms

* Materials with broad interests are welcome (e.g., tutorials form top conferences, high-quality surveys).

# For Presenters

 * Please fill your slots in the Agenda at least one week before your presentation.
    * Please format Paper fields with [venue+year]title (e.g. [ACL19]A Good Paper).
    * Please upload your slides, and add links to them in Slides fields.
 * Besides technical novelties, please give enough background knowledge in case people are unfamiliar with your topic.
 * It would be great to keep your presentation within 50 min.

# For Audiences
 * Please read abstract/introduction sections before the seminar.
 
# Agenda

| Week | Date | Name | Paper | Materials |
|------|------|------|-------|-----------|
| 7 | 11.3 | 宋扬 | Hello World | slides |
|------|------|------|-------|-----------|


# Alternative Reading List

| Paper | Materials |
|-|-|
| Neural Machine Reading Comprehension: Methods and Trends | [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Neural%20Machine%20Reading%20Comprehension%20Methods%20and%20Trends.pdf) |
| | |
| Neural Reading Comprehension and Beyond |  [PDF](https://github.com/NLPcourse/Representation/blob/main/Alternative%20Reading%20List/Neural%20Reading%20Comprehension%20and%20Beyond.pdf) |
|-|-|
| Attention-over-Attention Neural Networks for Reading Comprehension | [PDF]() |
|-|-|
| FANG: Leveraging Social Context for Fake News Detection Using Graph Representation | [PDF]()|
|-|-|
| Sequence to Sequence Learning with Neural Networks | [PDF]() |
|-|-|
| Generative Adversarial Nets | [PDF]() |
|-|-|
| Attention is All You Need | [PDF]() |
|-|-|
| Language Models are Unsupervised Multitask Learners  | [PDF]() |
|-|-|
| A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models | [PDF]() |
|-|-|
| Pre-training of Deep Bidirectional Transformers for Language Understanding | [PDF]() |
|-|-|
| Multi-task Deep Neural Networks for Natural Language Undestandering | [PDF]() |
|-|-|
| Distributed Representations of Words and Phrases and Their Compositionality | [PDF]() |
|-|-|
| Efficient estimation of word representations in vector space | [PDF]() |
|-|-|
| Playing Atari with Deep Reinforcement Learning  | [PDF]() |
|-|-|
| Human Level Control through Deep Reinforcement Learning  | [PDF]() |
|-|-|


